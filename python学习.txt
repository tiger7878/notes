1、官网：https://www.python.org/
	选择python 3.6版本：Windows x86-64 executable installer
	windows平台的软件包：https://www.lfd.uci.edu/~gohlke/pythonlibs/
	
2、开发工具PyCharm
	网址：https://www.jetbrains.com/pycharm/download/#section=windows
	选择：Professional专业版
	
3、Mysql
	下载地址：https://dev.mysql.com/downloads/mysql/
	版本选择：MySQL Community Server 5.6
	数据库 字符集：utf8mb4 -- UTF-8 Unicode
			排序规则：utf8mb4_general_ci

4、虚拟环境
	windows环境：cmd下执行
		pip install virtualenv
		pip install virtualenvwrapper-win
	创建虚拟环境
		mkvirtualenv 虚拟环境名称	#默认的python版本创建虚拟环境
		mkvirtualenv -p D:\Python\Python36\python.exe	虚拟环境名称	#指定python版本
	退出虚拟环境：deactivate
	列出所有虚拟环境：workon
	进入某个虚拟环境：workon 虚拟环境名称
	删除虚拟环境：直接删除某个虚拟环境的目录即可
	配置虚拟环境目录：在系统环境变量中配置
		WORKON_HOME		E:\python\Envs
		
5、计算机网络
	书籍：计算机网络自顶向下方法（第六版）、TCP/IP协议族（第4版）
	动态ip：家里的网络一定时间会更换IP。一般不要封ip，没用
	url：统一资源定位符
	TCP协议：负责将需要传输的数据分解成一定长度的片段
	IP协议：将数据贴上标签，包含源IP地址和目标IP地址
	路由器：记录着与自己相邻的路由器地址，并形成一个表格-路由表，路由表是一个动态的数据库
	XMPP：聊天协议
	
6、基础数据类型
	r：多行字符串
	u：表示Unicode编码，不加它，中文就不能正常显示。
	字符串：
			字符串截取：利用的是切片：'ABCDEFG'[:3]、'ABCDEFG'[-3:]、'ABCDEFG'[::2]
			isinstance(x, str) 可以判断变量 x 是否是字符串
			x.upper()：把字符串x变成大写
			s.strip(rm)：删除 s 字符串中开头、结尾处的 rm 序列的字符。当rm为空时，默认删除空白符（包括'\n', '\r', '\t', ' ')
			s.index(y):字符串y在字符串s中的第一次出现的索引位置，如果字符串不存在就会报错
			s[start:end]：字符串截取，从start到end
			s[-1]：表示取最后一个字符
			
	布尔类型：Python把0、空字符串''和None看成 False，其他数值和非空字符串都看成 True
	
	list:[]有序集合，里面可以是各种类型的数据。
		示例：L = ['Adam', 'Lisa', 'Bart']
		通过索引获取值：L[0]，例如-1代表取最后一条
		append(data)总是把新的元素添加到list的尾部
		insert(index,data)：把元素插入指定位置
		pop()：删除最后一个元素
		pop(index)：删除指定位置的元素
		range(n)：切片获取0-n索引的元素
		L[0:3]：取前3个元素
		L[::2]：第三个参数表示每N个取一个，上面的 L[::2] 会每两个元素取出一个来，也就是隔一个取一个
		range(1,101)得到一个数列：[1, 2, 3, ..., 100]
		len(list)：获取list中元素的个数
		
	tuple：（）元组，一旦创建完毕，就不能修改
		只有1个元素的tuple定义时必须加一个逗号，来消除歧义
		示例：t = ('Adam', 'Lisa', 'Bart')
		通过索引获取值：t[0]
		只有一个元素时在最后加,来表示它是一个tuple
		len(t)：获取tuple中元素的个数
		
	缩进规则：具有相同缩进的代码被视为代码块
	
	if：
		if 、else 、elif 表达式后面都有冒号：
		if x:
			print('True')
		只要x是非零数值、非空字符串、非空list等，就判断为True，否则为False
	
	input()：接收用户的输入，字符串类型
		示例：birth = input('birth: ')
	
	for循环：
		示例：L = ['Adam', 'Lisa', 'Bart']
				for name in L:
					print name
	break：退出循环
	continue：跳过本次循环，进行下一次循环
	
	dict：字典（键值对），key不能重复，无序集合
		示例：d = {'Adam': 95,'Lisa': 85,'Bart': 59}
		len(d)：获取集合中元素的个数
		d[key]：获取value，如果获取不到就报错
		d.get(key)：获取value，如果获取不到返回None
		判断key是否存在：if 'Paul' in d:
		d[key]=value：向dict中更新/插入键值对
		d.pop(key)：删除key和value
		for key in d：循环遍历key
	
	defaultdict ：比dict更加好用
		参数是dict：defaultdict(dict)
		参数是list：defaultdict(list)
		获取值时给默认值：dict.get('a','cc') 当a没有值时，默认返回cc
		defaultdict还可以接受一个方法名称，该方法返回dict
	
	json：
		json.dumps(data)：用于将字典形式的数据转化为字符串
		json.loads(data)：用于将字符串形式的数据转化为字典
		如果直接将dict类型的数据写入json文件中会发生报错，因此在将数据写入时需要用到json.dump(),json.load()用于从json文件中读取数据.
		将json字符串转换为list
			import ast
			nodes_list = ast.literal_eval(nodes_str)
	
	set：没有重复元素，无序集合，区分大小写
		示例：s = set(['A', 'B', 'C'])
		'A' in s：判断set中是否有A
		set中必须是不变对象，任何可变对象不能放入set中
		for name in s: 遍历元素
		s.add(value)：添加元素，如果已经存在，也不会报错
		s.remove(value)：删除元素，如果删除元素不存在，会报错，所以删除前需要判断
		& ：求两个set的交集
		| ：求两个set的合集
	date：日期处理	
		from datetime import datetime
		datetime.strptime(create_time_str, "%Y-%m-%d %H:%M:%S") #把日期字符串转换为日期
		
	 
7、函数
	定义一个函数要使用 def 语句，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用 return 语句返回
	实例：def my_abs(x):
	如果没有return语句，函数执行完毕后也会返回结果，只是结果为 None.return None可以简写为return。
	return x,y：表示返回多个值
	默认参数：def power(x, n=2): 。第二个参数可传，可不传。默认参数只能定义在必需参数的后面。
	常用函数：
		abs：接受一个参数，求绝对值
		max：接受多个参数，求最大值
		int：转换为int类型
		float：转换为float类型
		str：转换为字符串类型
		bool：转换为bool类型
		hex()：函数把一个整数转换成十六进制表示的字符串
	*args可变参数：表示的就是将实参中按照位置传值，多出来的值都给args，且以元祖的方式呈现
		示例：def fn(*args):
				print(args)
		传参：fn(1,2,3,4)
		得到：(1,2,3,4)
	**kwargs：是形参中按照关键字传值把多余的传值以字典的方式呈现
		示例：def foo(**kwargs):
					print(kwargs)
		传参：foo(y=1,a=2,b=3,c=4)
		得到：{'y': 1, 'a': 2, 'b': 3, 'c': 4}
	
	全局变量：在方法外定义的是全局变量，想在方法内部对全局变量赋值，需要先在方法内部使用global关键字，然后就可以赋值了
		例如：global my_driver
			  my_driver='xxx'
	
8、迭代
	索引迭代： enumerate()
		实例：L = ['Adam', 'Lisa', 'Bart', 'Paul']
				for index, name in enumerate(L):
					print index, '-', name
	zip函数合并： 
		实例：L = zip([10, 20, 30], ['A', 'B', 'C']) 得到的是[(10, 'A'), (20, 'B'), (30, 'C')]
				for index, name in L:
					print(index, name)
	迭代dict的value：
		示例：
		d = { 'Adam': 95, 'Lisa': 85, 'Bart': 59 }
		for v in d.values():  #占内存，d.values()得到list
		for v in d.itervalues(): #不占内存
	
	迭代dict的key和value：
		示例：
			d = { 'Adam': 95, 'Lisa': 85, 'Bart': 59 }
			for key, value in d.items():
			print key, ':', value
			
9、生成列表
		range(1, 11)得到：[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
		[x * x for x in range(1, 11)]得到：[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
		range(1, 100, 2)得到：[1, 3, 5, 7, 9,...]
		[x * x for x in range(1, 11) if x % 2 == 0]得到：[4, 16, 36, 64, 100]

10、类：
	关键字：class
	类名：类名首字母大写
	实例：同一个类的每个实例拥有各自不同的属性，但是，现实世界中，一种类型的实例应该拥有相同名字的属性
	构造函数：__init__(self,xx,xx)，第一个参数必须是 self
	任意关键字参数:**kw
		def __init__(self, name, gender, birth, **kw):
			self.birth = birth
			for k, v in kw.iteritems():
				setattr(self, k, v)
	属性：通常是没有访问控制,__双下划线开头的通常是不能被外部访问
	类的属性：定义在类里面，类属性有且只有一份，通过类名进行访问。相当于java中的静态变量
	实例属性：定义在__init__方法中，实例属性每个实例各自拥有，互相独立，通过对象名进行访问
	属性优先级：当实例属性和类属性重名时，实例属性优先级高
	打印所有属性：dir(对象)
	实例方法：在类中定义的函数，它的第一个参数永远是 self
	类方法：注意类方法需要添加 @classmethod，通过类名调用，相当于java中的静态方法
	type() 函数获取变量的类型
	getattr(s, 'name')  # 获取s对象的name属性
	getattr(s, 'age', 20)  # 获取age属性，如果属性不存在，就返回默认值20
	setattr(s, 'name', 'Adam')  # 设置s对象新的name属性
	
11、高阶函数
	概念：把方法名作为函数的参数
	map函数：接收一个函数 f 和一个 list，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 并返回。
		示例：
		def f(x):
			return x*x
		print map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])
		得到：
		[1, 4, 9, 10, 25, 36, 49, 64, 81]
		
	reduce函数：接收的参数和 map()类似，一个函数 f，一个list，传入的函数 f 必须接收两个参数，对list的每个元素反复调用函数f，并返回最终结果值
		示例：
		def f(x, y):
			return x + y
		reduce(f, [1, 3, 5, 7, 9])
		得到：25
		该函数可以有3个参数。第三个参数作为初始的值
		
	filter函数：接收一个函数 f 和一个list，这个函数 f 的作用是对每个元素进行判断，返回 True或 False，据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list。
	
	sorted函数：接收一个比较函数来实现自定义排序，比较函数的定义是，传入两个待比较的元素 x, y，如果 x 应该排在 y 的前面，返回 -1，如果 x 应该排在 y 的后面，返回 1。如果 x 和 y 相等，返回 0
		示例：
		def reversed_cmp(x, y):
			if x > y:
				return -1
			if x < y:
				return 1
			return 0
		sorted([36, 5, 12, 9, 21], reversed_cmp)
		得到：
		[36, 21, 12, 9, 5]
		sorted([36, 5, 12, 9, 21])得到：[5, 9, 12, 21, 36]。
		sorted(['bob', 'about', 'Zoo', 'Credit'])得到：['Credit', 'Zoo', 'about', 'bob']。字符串进行排序，字符串默认按照ASCII大小来比较
		
		list中的对象根据name属性排序：
		class Person(object):
			pass
		p1 = Person()
		p1.name = 'Bart'
		L1 = [p1, p2, p3]
		sorted(L1, lambda p1, p2: cmp(p1.name, p2.name))
	
	python中返回函数：函数内部定义一个函数，然后返回该函数名
		示例：
			def f():
				print 'call f()...'
				# 定义函数g:
				def g():
					print 'call g()...'
				# 返回函数g:
				return g
	
12、装饰器decorator
	用于增加函数的功能，简化代码，避免每个函数都写一遍。相当于java中的AOP
	打印日志：@log
	监测性能：@performance
	数据库事务：@transaction
	URL路由：@post('/xxx')
	
13、模块和包
	模块：一个py文件就是一个模块。不同的模块下面可以有同名的函数和变量，互相不影响
		test.py的模块名称是test
		import test 就引入了test模块
		test.xx() 调用test模块下的xx方法
	包：文件夹就是包，不同的包下面可以有同名的模块，互相不影响
		包下面必须有__init__.py文件，即使该文件是个空文件。每一层都必须有
	动态导入：先尝试从cStringIO导入，如果失败了（比如cStringIO没有被安装），再尝试从StringIO导入
		try:
			from cStringIO import StringIO
		except ImportError:
			from StringIO import StringIO
	__future__：旧版本测试新版本的特性
	安装第三方模块：pip install xx.py。第三方模块搜索的网址：https://pypi.org/
	常用包：math、json、threading
	
14、类的继承
	在子类的__init__方法中，需要调用父类的super(子类, self).__init__()方法
	isinstance(变量,类)判断类型变量是否是这个类型
	多重继承：python允许多重继承
	
15、特殊方法
	魔术方法：__xx__，两个下划线的方法，默认有好多
	打印对象：重写__str__方法，否则就是打印出对象的类名
	比较函数：cmp(x,y)方法，排序的时候默认调用它
	获取长度：len方法，获取元素个数，
	关联函数要实现时必须都实现：__getattr__ 、__setattr__ 、 __delattr__
	get方法：用@property装饰。有get没有set就是一个只读属性
	set方法：用@属性.setter装饰，在这里进行值有效性校验
		@property
		def score(self):
			return self.__score
		@score.setter
		def score(self, score):
			if score < 0 or score > 100:
				raise ValueError('invalid score')
			self.__score = score
	限制添加属性：__slots__
		__slots__ = ('name', 'gender', 'score')
	__call__：重写该方法可以使一个对象被调用
	
16、字符串格式化
	编码：# -*- coding: utf-8 -*-
	格式化的字符串：'(I am %s、I am %d years old)' % ('Tom', 18) 得到的是：(I am Tom、I am 18 years old)
	%d：整数
	%f：浮点数
	%s：字符串，它会把任何数据类型转换为字符串
	%x：十六进制整数
	%%：字符串里面的%是一个普通字符怎么办？这个时候就需要转义，用%%来表示一个%
	format：用传入的参数依次替换字符串内的占位符{0}、{1}..，不过这种方式写起来比%要麻烦得多
		'Hello, {0}, 成绩提升了 {1:.1f}%'.format('小明', 17.125)，得到：'Hello, 小明, 成绩提升了 17.1%'。
		'xx{}xxx{}'.format(a,b) 这样也可以，不用数字
		
17、HTTP协议
	它本质还是一串字符串
	content-type:
		form表单提交：application/x-www-form-urlencoded
		文件上传：multipart/form-data
		json格式：application/json
		
18、requests
	它是Python的http库
	文档：http://docs.python-requests.org/zh_CN/latest/user/quickstart.html
	安装：pip install requests
	豆瓣源安装：pip install requests -i "https://pypi.doubanio.com/simple/"
	使用requests是完全可以伪装成一个浏览器，来做get、post等http请求
	get请求：
	# res = requests.get(url)
	# res = requests.get(url, params=param)
	# res = requests.get(url, headers=my_headers)
	# print("返回的内容：" + res.text)
	# print("编码：" + res.encoding)
	# print("状态码：%d" % res.status_code)
	# print(res.headers)

	post请求：
	# data：字符串
	# json：json数据
	# res = requests.post(url, data=param)
	# res = requests.post(url, data=json.dumps(param).encode("utf8"))
	res = requests.post(url, json=param, headers=my_headers)
	print("返回的内容：" + res.text)
	# 把响应的数据转换为json
	print(res.json())
	
	http响应码说明：
	2xx：成功
	3xx：跳转（重定向）
	4xx：客户端错误
	5xx：服务器端错误
	
	小结：因为http是无状态的，所以通过request是可以模拟浏览器发起请求，服务器是区分不了的
	
19、正则表达式
	引入包：re
	math方法：从字符串最开始匹配的，返回的是一个对象，并且只会去第一行查找，多行匹配不能用它
	sub方法：字符串替换，原始的字符串是不变的
	search：不是从字符串最开始匹配，多行数据匹配可以用它
	re.IGNORECASE：忽略大小写匹配
	
20、beautifulsoup4
	官网：https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html
	安装：pip install beautifulsoup4
	引入：from bs4 import BeautifulSoup
	使用：bs = BeautifulSoup(html, "html.parser")
		# 根据id查找
		# div_tag = bs.find(id="list")
	find方法：
		# 查找到第一个div
		# div_tag = bs.find("div")
		# 根据id查找
		# div_tag = bs.find(id="list")
		#  查找id=list的div
		# div_tag = bs.find("div", id="list")
		# 通过正则表达式
		# div_tag = bs.find("div", id=re.compile("info-\d+"))
		# 根据字符串来查找
		# div_tag = bs.find(string="胜多负少121")
		# 正则匹配html标签中的内容
		div_tag = bs.find(string=re.compile("胜多"))
		print(div_tag)
	find_all方法：
		#查找所有的div
		bs.find_all("div")
	contents：获取一级子元素
	descendants：获取所有的子元素，子元素的子元素也能获取
	parent：获取父元素
	parents：获取所有的父元素
	获取属性：
		p_tag["id"]
		p_tag.get("id")
	说明：回车换行符也是一个元素，所以经常会获取到None
	匹配多行文本：记得在参数里面加re.DOTALL
	
21、xpath
	它比beautifulsoup4更好，可以使得解析变成可配置的解析，也就是把xpath配置起来，对方网站变了以后，只需要改解析配置就好
	内置方法：https://developer.mozilla.org/en-US/docs/Web/XPath/Functions
	windows安装scrpay
		先安装c++ buildtools：https://www.microsoft.com/en-us/download/details.aspx?id=48159
		安装lxml：pip install lxml
		安装Twisted：pip install twisted
		
		另一个安装网址：https://www.lfd.uci.edu/~gohlke/pythonlibs/，搜索lxml、twisted、scrapy。注意版本
			本项目的终端下：pip install E:\python\package\Twisted-19.2.0-cp36-cp36m-win_amd64.whl
					pip install E:\python\package\Scrapy-1.6.0-py2.py3-none-any.whl
			
	引入包：from  scrapy import Selector
	
	sel = Selector(text=html)
		# 获取到元素，用索引来控制某一个，取到的可能是多个
		tag = sel.xpath('//*[@id="myP"]').extract()[0]
		print(tag)
		# 获取元素里面的值text()
		tag_text = sel.xpath('//*[@id="myP"]/text()').extract()[0]
		print(tag_text)
	
		# 属性全值匹配
		# teacher_tag = sel.xpath("//div[@class='teacher_info info']")
		# 属性部分值包含
		# teacher_tag = sel.xpath("//div[contains(@class,'teacher_info')]")
		# 获取最后一个元素
		# teacher_tag = sel.xpath("//div[contains(@class,'teacher_info')]/form/input[last()]").extract()
		# 获取value属性的值
		# teacher_tag = sel.xpath("//div[contains(@class,'teacher_info')]/form/input[last()]/@value").extract()
		# print(teacher_tag)
		# 获取多个元素
		# div_tag = sel.xpath('//p[@class="pxx"]|//p[@class="pcc"]').extract()
		# 获取多个元素里面的值
		div_tag = sel.xpath('//p[@class="pxx"]/text()|//p[@class="pcc"]/text()').extract()
		print(div_tag)
		# 获取id以post开始的div
		all_divs = sel.xpath("//div[starts-with(@id,'post-')]")
		备注：有text()就是获取值，没有text()就是获取html
	
	css选择器：
		sel = Selector(text=html)
		# 通过class获取元素
		# tag = sel.css(".teacher_info")
		# 通过id获取元素
		# tag = sel.css("#myP")
		# 获取第一个元素
		# tag = sel.css("#ixc > p")
		# 获取元素的值text
		# value = sel.css("#ixc > p::text").extract()[0]
		# 获取第二个p元素，这里的数字4是这个元素所在的位置
		# tag = sel.css("#ixc > p:nth-child(4)")
		# 获取本元素后面的一个元素
		# tag = sel.css(".teacher_info + p")
		# 获取本元素后面的相邻的多个元素
		# tag = sel.css(".teacher_info ~ p")
		# 根据属性值提取元素
		# tag = sel.css("a[href='http://www.baidu.com']::text").extract()[0]
		# 属性包含提取元素
		# tag = sel.css("a[href*='baidu']::text").extract()
	
22、实战一抓取静态网站
	①制定策略（如何一步一步获取数据）
	②分析网站时静态还是动态
	③明确抓取数据
	④思考的问题：
		1、将获取初始的url放到单独的线程中去完成
		2、如何停止线程
		3、手动停止线程：ctrl+c
			①接收ctrl+c
			②保存临时变量
			③下次启动程序的时候，我们应该如何从文件（mysql）中启动
		4、程序有可能直接崩溃
			① queue是内存中的，是否安全
			② queue这种模式就不合适了，使用mysql来同步
			③ redis
		5、爬虫完成以后的退出
			如何去判断爬虫数据已经抓取完成了
	域名拼接：原本路径有域名就不变，没有域名就加上域名
		from urllib import parse
		domain = "https://bbs.csdn.net"
		url="/about/me"
		parse.urljoin(domain, url)
	
23、pymysql
	官网：https://pymysql.readthedocs.io/en/latest/
	连接mysql的驱动
	新建数据库：spider
		字符集：utf8mb4 -- UTF-8 Unicode、排序规则：utf8mb4_general_ci  #有表情符号时用它
	安装：pip install PyMySQL
	导入包：import pymysql.cursors
	
24、peewee
	一个轻量级的ORM框架
	官网：http://docs.peewee-orm.com/en/latest/  查看Quickstart有示例
	安装：pip install peewee
	获取单条：Person.select().where(Person.name == 'Tom').get() #获取不到数据会抛出异常
	获取所有：Person.select().where()
	新增、修改：person.save()
	删除：person.delete_instance()
	自定义id作为主键并赋值时：topic.save(force_insert=True)
	字段说明verbose_name：praised_nums = IntegerField(default=0, verbose_name="点赞数")
	
25、多线程Thread
	并行：同一个时间点同时运行，指的是多个CPU
	并发：一个时间段执行，指的是一个CPU
	导入包：from threading import Thread
	方式一：实例化Thread
		t1 = Thread(target=sleep_task)
	方式二：继承Thread类
		class SleepThread(Thread):
		t1 = SleepThread()
	join方法：等待线程执行完成再执行后面的方法，可以测试到多线程的真实耗时
	setDaemon方法：主线程结束，子线程也结束
	GIL：全局解释器锁，CPU在同一个时间只会执行一个线程。也就是GIL无法利用多CPU
		释放：时间片释放、io释放，所以对性能影响不大
		它是cpython（python的解释器）的产物
	IO操作：操作数据库、请求网络等，web系统中最占时间的部分
	
	同步锁：保证代码段完整执行，比如购买火车票
		from threading import Lock
		total_lock = Lock()
		# 加锁
		total_lock.acquire()
		# 释放锁
		total_lock.release()
		
		# 程序执行完成才往下执行
		t1.join()
		t2.join()
		print(total)
		
	Queue：线程间通信，它是线程间同步的，线程安全的
		导包：from queue import Queue
		对象：my_queue = Queue()
		# 创建queue时指定最大长度，超过这个长度的就不能存放进去，控制并发用
		message_queue = Queue(maxsize=2)
		# 存数据，会线程阻塞
		message_queue.put("Tom")
		message_queue.put("Jack")
		# 设置了timeout后，就会抛出异常，不会被阻塞
		message_queue.put("Rose", timeout=2)
		# 添加不进去就立刻抛出异常，不用等待
        message_queue.put_nowait("Rose")
		# 取数据，先进先出，会线程阻塞
		text = message_queue.get()
		print(text)
		# 取数据时线程不等待，没有数据就报错
		text = message_queue.get_nowait()
		
	ThreadPoolExecutor：线程池
	
26、动态网站抓取
	思路：使用谷歌浏览器，按Ctrl+U，然后把代码复制到一个html文件，然后删除里面的js文件，就可以发现哪些是动态加载的，哪些是静态的
	XHR：谷歌浏览器中Network→XHR→代表异步请求
	url分析：尝试删除url中没有用的参数

27、python虚拟环境
	windows下安装：pip install virtualenvwrapper-win
	非windows环境下安装：pip install virtualenvwrapper
	创建环境变量：WORKON_HOME E:\python\Envs。如果不创建，默认的虚拟环境会安装到：C:\Users\Administrator.WIN7U-20140512U\Envs，不用添加到Path中
	cmd创建虚拟环境：mkvirtualenv --python=D:\Python\Python36\python.exe 自定义虚拟环境名称。创建以后就自动进入虚拟环境了。
	退出虚拟环境：deactivate
	列出虚拟环境：workon（在E:\python\Envs目录下使用cmd命令）
	进入某个虚拟环境：workon 虚拟环境名称
	删除虚拟环境：直接删除某个虚拟环境的目录即可，或者 rmvirtualenv 某个虚拟环境名称
	查看安装的依赖包：pip list

28、模拟登录保存cookie
	requests获取cookie
		pickle：一个不错的工具，将对象写入到文件中，比如cookie保存到文件中
		import pickle
		# post进行登录
		res = requests.post(url=url, data=post_data, headers=headers)
		#cookie保存到文件
		# 把cookie保存到文件中，这里是相对路径，可以写全路径
        with open("douban.cookie", "wb") as f:
            pickle.dump(res.cookies, f)
		
		#从文件中读取cookie
		with open("douban.cookie", "rb") as f:
			cookies = pickle.load(f)
			
	selenium登录豆瓣
		url = "https://www.douban.com/"
		# 打开一个url地址获取数据
		browser.get(url)
		time.sleep(3)
		# 由于登录框是在iframe中，所以要切换
		browser.switch_to_frame(browser.find_element_by_tag_name("iframe"))
		# 获取cookies
		cookies = browser.get_cookies()
		# 把cookie保存到dict中
		cookie_dict = {}
		for item in cookies:
			cookie_dict[item["name"]] = item["value"]
		# 使用selenium获取的cookie来使用requet访问地址，测试是否登录
		html = requests.get("https://www.douban.com/", headers=headers, cookies=cookie_dict).text
		
	selenium登录拖动验证码
		屏幕截图时一定要保证操作系统的显示里面的缩放与布局设置是100%，否则有问题
		
	
29、scrapy
	29.1、scrapy安装：
	方式一：安装 Microsoft visual c++ 14.0（E:\python\tool\visualcppbuildtools_full.exe）
	从豆瓣源来安装：pip install -i https://pypi.douban.com/simple/ scrapy
	如果安装失败就去找文件：https://www.lfd.uci.edu/~gohlke/pythonlibs/，搜索lxml、twisted、scrapy。注意版本.
	找到文件以后，执行一下下载的文件的安装，然后再执行从豆瓣安装的命令。
	
	方式二：
	安装buildtools（E:\python\tool\BuildTools_Full.exe）
	pip install E:\python\package\lxml-4.3.3-cp36-cp36m-win_amd64.whl
	pip install E:\python\package\Twisted-19.2.0-cp36-cp36m-win_amd64.whl
	pip install E:\python\package\Scrapy-1.6.0-py2.py3-none-any.whl
	
	29.2、scrapy项目构建
	文档：百度 scrapy document 或者scrapy 文档 https://scrapy-chs.readthedocs.io/zh_CN/latest/
	创建虚拟环境：mkvirtualenv --python=D:\Python\Python36\python.exe article_spider（在Envs目录下）
	安装scrapy
	创建scrapy项目：
		在项目文件存放路径下打开cmd
		workon：列出所有虚拟环境
		workon atricle：切换到atricle虚拟环境
		scrapy startproject ArticleSpider
	然后用pycharm打开项目
	在终端中（或者在上面的cmd中cd到 ArticleSpider目录下）
		scrapy genspider jobbole blog.jobbole.com：创建一个爬虫项目jobbole，爬取blog.jobbole.com网站
		此时就发现多了一个jobbole.py文件
	在pycharm中选择article_spider虚拟环境：
		settings→Project Interpreter
	在终端中安装pypiwin32（windows下才需要）
		pip install -i https://pypi.douban.com/simple/ pypiwin32
	项目调试：
		在scrapy.cfg同级创建main.py文件
		# 项目调试
		from scrapy.cmdline import execute

		import sys
		import os

		# 把当前项目名称添加进去
		# 获取当前文件的相对路径：os.path.abspath(__file__)
		# 获取当前文件所在目录的绝对路径：os.path.dirname(os.path.abspath(__file__))
		sys.path.append(os.path.dirname(os.path.abspath(__file__)))
		# 执行命令
		execute(["scrapy", "crawl"])
	在终端中运行项目：scrapy crawl jobbole ，若发现运行不报错，就证明没问题了，这就是终端运行一个项目
	修改main.py文件：execute(["scrapy", "crawl", "jobbole"])
	修改settings.py文件：ROBOTSTXT_OBEY = False #不遵循这个爬虫协议，大网站才有这种协议
	然后右键debug我们的main.py文件
	在终端或者cmder中执行：scrapy shell http://blog.jobbole.com/110287 ，这样获取一次url中的内容可以不停的调试，防止每次调试都去获取
	items.py：数据项类，可以用来传递到pipelines.py文件中。（需要在settings.py文件中打开：ITEM_PIPELINES节点）
	meta传递参数：
		传递方：yield Request(url=parse.urljoin(response.url, post_url), meta={"front_image_url": image_url}, callback=self.parse_detail)
		接收方：front_image_url = response.meta.get("front_image_url", "")  # 封面图，从response中获取
	
	图片保存：
		在settings.py中引入：import os
		在终端中执行命令安装：pipllow
			pip install -i https://pypi.douban.com/simple pillow
		在settings.py文件中配置ITEM_PIPELINES
			'scrapy.pipelines.images.ImagesPipeline': 1  #数字越小就越先处理
		在工程目录下创建文件夹：images
		在ITEM_PIPELINES的同级配置：
			# 图片字段名称
			IMAGES_URLS_FIELD = "front_image_url"
			# 项目绝对路径
			project_dir = os.path.abspath(os.path.dirname(__file__))
			# 图片存储路径
			IMAGES_STORE = os.path.join(project_dir, "images")
		
	定制图片的pipeline
		在pipeline.py文件中：自定义pipeline类，重载item_completed函数，文件存储路径在results参数中，已下载的文件不会重复下载。
	
	mysql相关：
		mysql的驱动安装：pip install -i https://pypi.douban.com/simple/ mysqlclient
			如果是win32的python：
				那么上面安装有可能失败（MySQLdb/_mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory）
				就需要到：https://www.lfd.uci.edu/~gohlke/pythonlibs/#mysqlclient 下32位的whl来安装
					pip install E:\python\mysqlclient-1.4.2-cp37-cp37m-win32.whl
		centos下安装驱动：yum install python-devel mysql-devel
		保存到数据库分为同步和异步两种操作，异步操作是Twisted框架提供的连接池的方式。异步使得mysql的插入不再是爬取时的瓶颈
		自定mysql的同步插入pipelines：MysqlPipeline
		需要在settings.py中的ITEM_PIPELINES节点中配置:
			'ArticleSpider.pipelines.MysqlPipeline': 1
		异步操作：
			from twisted.enterprise import adbapi #异步操作归功于它
			import MySQLdb
			import MySQLdb.cursors #这个如果不引入会报错
			报错：MySQLdb and (2006, 'MySQL server has gone away') 的解决方案是：加入 cp_reconnect=True
				self.pool = adbapi.ConnectionPool("MySQLdb", self.parms['host'], self.parms['username'], self.parms['password'], self.parms['database'], cp_reconnect=True)
		ORM框架：scrapy-djangoitem，github中有源码
		
	ItemLoader：
		在这个容器里面可以配置item中各个字段的提取规则。可以通过函数分析原始数据，并对Item字段进行赋值，非常的便捷
		导包：
			from scrapy.loader import ItemLoader
			from scrapy.loader.processors import TakeFirst, MapCompose, Identity, Join
		自定义ItemLoader，继承自ItemLoader
			class ArticleItemLoader(ItemLoader):
				# 实现之前是在没一项解析时写extract_first()方法
				# 这里只是重载这个属性，设置为只选取第一个值
				# TakeFirst()是Scrapy提供的内置处理器，用于提取List中的第一个非空元素
				default_output_processor = TakeFirst()
		input_processor：输入处理器，当这个字段的值传过来时，可以在传进来的值上面做一些预处理
		output_processor：输出处理器，这个字段被预处理完之后，输出前最后的一步处理
		scrapy内置的处理器：
			TakeFirst()：返回第一个非空（non-null/ non-empty）值，常用于单值字段的输出处理器，无参数
			Identity()：最简单的处理器，不进行任何处理，直接返回原来的数据。无参数
			Join()：返回用分隔符连接后的值。分隔符默认为空格。不接受Loader contexts。
			Compose()：
				用给定的多个函数的组合，来构造的处理器。list对象（注意不是指list中的元素），依次被传递到第一个函数，然后输出，再传递到第二个函数，一个接着一个，直到最后一个函数返回整个处理器的输出。
				默认情况下，当遇到None值（list中有None值）的时候停止处理。可以通过传递参数stop_on_none = False改变这种行为。
			MapCompose()：
				与Compose处理器类似，区别在于各个函数结果在内部传递的方式（会涉及到list对象解包的步骤）
				输入值是被迭代的处理的，List对象中的每一个元素被单独传入，第一个函数进行处理，然后处理的结果被连接起来形成一个新的迭代器，并被传入第二个函数，以此类推，直到最后一个函数。最后一个函数的输出被连接起来形成处理器的输出
				每个函数能返回一个值或者一个值列表，也能返回None（会被下一个函数所忽略）
				这个处理器提供了很方便的方式来组合多个处理单值的函数。因此它常用于输入处理器，因为传递过来的是一个List对象。
		简单示例：
			# 注意第一个参数必须是一个对象，并不是类名
			item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response)
			# 装入Item
			article_item = item_loader.load_item()
			
			# 剔除“评论”
			def tags_convert(value):
				if "评论" in value:
					return ""
				else:
					return value
			JobBoleArticleItem里面的字段：	
			tags = scrapy.Field(
				# 剔除评论
				input_processor=MapCompose(tags_convert),
				# 以逗号连接输出，使用processor自带的Join拼接方法
				output_processor=Join(",")
			)
		
30、反扒技术
	30.1、自动切换user_agent
		scrapy的官网：http://doc.scrapy.org/en/latest/
		middleware文档：http://doc.scrapy.org/en/latest/topics/downloader-middleware.html
		随机获取useragent的插件：https://github.com/hellysmile/fake-useragent
		安装：pip install fake-useragent
		导入依赖包：from fake_useragent import UserAgent
		
		首先在middlewares.py文件中添加RandomUserAgentMiddleware类：
			# 自定义随机切换useragent
			class RandomUserAgentMiddleware(object):

				def process_request(self, request, spider):
					# UserAgent().random 就是随机生成一个 UserAgent
					request.headers.setdefault('User-Agent', UserAgent().random)
		
		然后在settings.py文件中
			打开DOWNLOADER_MIDDLEWARES节点，并添加如下配置：
			DOWNLOADER_MIDDLEWARES = {
				# 随机切换useragent
				'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543,
				# 将scrapy默认的middleware设置为none，否则会覆盖我们自定义的
				'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
			}
		
		# 添加默认的USER_AGENT，即使没有随机切换，就可以用这个默认的
		USER_AGENT = "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36"
		完善：高级做法配置文件中配置随机的类型（学习资料里面有）
		
	30.2、IP代理服务器
		免费的：很多人用，不稳定
			https://www.xicidaili.com/nn/，选择高匿代理（不太稳定，很多人用）
			https://www.kuaidaili.com/free/inha/1/
			http://ip.zdaye.com/dayProxy.html
			http://www.qydaili.com/free/?action=china&page=1
			pip install requests
			在settings.py的DOWNLOADER_MIDDLEWARES节点配置我们写的middlewares
			开源代码：https://github.com/aivarsk/scrapy-proxies（参考它改动自己的）
			收费项目：https://github.com/scrapy-plugins/scrapy-crawlera（比免费的要稳定，只需要拿到key，不用自己维护ip池，收费的建议使用它）
			收费ip代理：https://www.abuyun.com/ 
			tor：洋葱网络，比较安全，需要配合vpn来使用，也会隐藏我们的ip。比我们用的免费和收费的ip代理都稳定。
		收费的：
			阿布云：费用比较贵 https://www.abuyun.com/  随机取的ip，用的ip可能失效，获取失败就try一下多尝试几次就够我们使用了，需要登录的网站就没用
			
	
	30.3、其他反扒手段
		禁用cookie：（在settings.py中打开配置文件并修改：）
			COOKIES_ENABLED = False #禁用cookie，有些网站需要禁用，有些不能禁用
		自动限速：（在settings.py中打开配置文件并修改：）
			文档：https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html?highlight=%E8%87%AA%E5%8A%A8%E9%99%90%E9%80%9F
			AUTOTHROTTLE_ENABLED = True #启用AutoThrottle扩展
			AUTOTHROTTLE_START_DELAY = 5 #初始下载延迟(单位:秒)
			DOWNLOAD_DELAY = 10 #下载延迟
		自定义settings：（在自己写的spider类中写属性，这样每个爬虫就有自己的配置）
			custom_settings={
				"COOKIES_ENABLED" = True
			}
		Referer：记录的从哪个地址跳转过来到本链接地址的，这个也很重要
		比对headers：看自己的和原网站的区别在哪里
		
33、selenium：
	安装：pip install selenium
	官方文档：https://selenium-python.readthedocs.io/index.html
	下载Drivers：https://selenium-python.readthedocs.io/installation.html → 1.3 Drivers → 选择Chrome→根据自己装从chrome选择版本，一定要对应
		如果访问不了，记得开代理
	第二种下载Drivers的方法：http://npm.taobao.org/mirrors/chromedriver 淘宝的连接，这里的版本号要跟chrome的一致
	基本用法：
		import time
		from scrapy import Selector
		from selenium import webdriver
		
		# 初始化，设置chromedriver的位置
		browser = webdriver.Chrome(executable_path="E:/python/chromedriver/chromedriver-79.exe")
		# 打开一个url地址获取数据
		browser.get("https://item.jd.com/100004770235.html#none")

		# 点击页面上的元素 ，每次点击元素以后，先延迟几秒然后重新创建下Selector，因为点击后页面内容不同了
		click_ele = browser.find_element_by_xpath("//li[@clstag='shangpin|keycount|product|shangpinpingjia_1']")
		click_ele.click()

		# 获取运行js之后的html，包含了静态和动态的值
		html = browser.page_source
		print(html)

		# 使用xpath提取元素
		sel = Selector(text=html)
		price = sel.xpath("//span[@class='price J-p-100004770235']/text()").extract_first()
		print("获取到价格：{}".format(price))

		time.sleep(20)

		# 关闭浏览器
		browser.close()
	如果有些网站识别了ChromeDriver时需要做如下操作：手动启动chrome
		首先找到本机chrome.exe的文件位置，然后cmd到那里
		然后关闭所有的chrome
		然后执行：chrome.exe --remote-debugging-port=9222
		在打开的chrome中输入：http://127.0.0.1:9222/json  ，当看见内容表示成功
		重写 start_requests()方法
	python控制鼠标：https://github.com/boppreh/mouse
		安装：pip install mouse
		引入包：from mouse import move,click
		move(x,y) #鼠标移动到x、y坐标的位置
		click() #鼠标点击
	屏幕坐标获取工具：http://www.greenxiazai.com/soft/35439.html
	获取浏览器里面的cookie：
		先在settings.py中把COOKIE_ENABLE=True；COOKIE_DEBUG=True。还要有user-agent
		#保存cookie到文件
		import json
		cookies=browser.get_cookies()
		jsonCookies = json.dumps(cookies)
		with open('../cookies/qccNLCookie.json', 'w') as f:
			f.write(jsonCookies)
			
		#从文件中读取cookie
		import json
 
		str=''
		with open('qqhomepage.json','r',encoding='utf-8') as f:
			listCookies=json.loads(f.read())
		cookie = [item["name"] + "=" + item["value"] for item in listCookies]
		cookiestr = '; '.join(item for item in cookie)
		print(cookiestr)
	selenium 控制元素：
		source_url = driver.find_element_by_xpath('//span[@class="from"]/a').get_attribute("href")
	Selector 控制元素：（from scrapy import Selector  #它更加的高效率）
		page_source = driver.page_source
        selector = Selector(text=page_source)
        content = selector.xpath('//div[contains(@class,"news-content")]').extract_first('')
	selenium找不到元素：
		可以尝试在selenium.get("http://xxx.html") 后 time.sleep(2)，然后再获取元素
	执行javascript代码：模拟鼠标下拉
		#执行一次下拉，如果要执行多次就写for循环，每次记得sleep
		browser.excecute_script("window.scrollTo(0,document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;") 
	chrome不加载图片：（有的时候加载图片很慢，可以加速请求）
		chrome_opt=webdriver.ChromeOptions()
		prefs = {"profile.managed_default_content_settings.images":2}
		chrome_opt.add_experimetal_option("prefs",prefs)
		browser = webdriver.Chrome(executable_path="E:/python/chromedriver.exe",chrome_options=chrome_opt)
	chrome也可以在无界面的情况下使用(便于在linux中使用)：
		from selenium.webdriver.chrome.options import Options
		# 配置
		chrome_options = Options()
		# 设置headless模式
		chrome_options.add_argument("--headless")
		# 防止一些bug出现
		chrome_options.add_argument("--disable-gpu")
		# 设置不加载图片
		chrome_options.add_argument("blink-settings=imagesEnabled=false")
		browser = webdriver.Chrome(executable_path="E:/python/chromedriver/chromedriver-79.exe", chrome_options=chrome_options)
	高性能方式：
		scrapy-splash提高并发性
		分布式selenium+grid提高并发性能
		
34、requests登录知乎
	cookie：python3 是 import http.cookiejar as cookielib
	session = requests.session() #长连接，不用每次请求都建立连接，效率高
	session.cookies=cookielib.LWPCookieJar(filename="cookie.txt") #这样使我们保存和读取cookie很容易
		session.get(url,allow_redirects=False) #get请求，用来控制请求未登录的页面时，设置为false就不会调整到登录页，判断用户是否登录用
		session.post(url) #post请求
	session.cookies.save() #保存cookie
	session.cookies.load(ignore_discard=True) #读取cookie
	
35、scrapy登录知乎
	 get请求：scrapy.Request
	 post请求：scrapy.FormRequst
	 登录成功以后，scrapy会把我们的cookie保存起来，以后的请求不需要加上cookie
	 url处理：from urllib import parse
			parse.join(response.url,'获取的href中的url') #这样就可以得到完整的url路径
			示例：[parse.join(response.url,url) for url in all_urls] #把all_urls的每个url都补成全路径
	 filter函数：用来过滤不需要的。
			示例：new_urls=filter(lambda x:True if x.startswith('https') else False,all_urls) #过滤出以https开头的url
	 yield：是传输出去的写法，不如说给下载器，给pipeline等
			可以同时存在多个yield。
			yield scrapy.Request()  #交给异步去请求
			yield item #交给pipeline
	start_requests：是整个爬虫的入口
	一个pipeline处理所有的item：
		方式一：获取item的classname来判断（不推荐）
			if item.__class__.__name__ == "xxxItem"
				xxsql
		方式二：利用Django的思想，在每个item里面定义get_inset_sql方法（注意字段不能错，否则sql报错），返回sql和params参数，
				然后在pipeline中调用item的方法获取sql和params参数，
				然后在pipelines中的mysql异步插入中修改do_insert方法
	日期格式化：在scrapy中从settings中获取配置文件
		在settings.py中定义：
			SQL_DATETIME_FORMART="%Y-%m-%d %H:%M%S"
			SQL_DATE_FORMART="%Y-%m-%d"
		在需要使用的文件中引用
			from settings import SQL_DATETIME_FORMART,SQL_DATE_FORMART
	
36、CrawlSpider爬取拉钩
	CrawlSpider：是一个整站爬取策略
	cmder工具：是windows下的一个模拟linux的工具，可以使用linux的部分命令，
		网址：https://cmder.net/  #选择Download Full
		创建环境变量指向安装路径
		cmder中选中文件后，按鼠标右键就可以复制
	cmder中查看默认的spider模板：scrapy genspider --list  #默认是basic模板
	虚拟环境中创建一个基于crawl模板的spider：scrapy genspider -t crawl laogou www.laogou.com  #如果报错就要看视频6-2进行配置
	注意：不能重载parse函数，因为它本身就重载了
	核心函数：self._parse_response
	cb_kwargs：传递给link_extractor的参数
	restrict_xpaths：对url进行xpath限定，控制只抓取部分url
	restrict_css：对url进行css限定，控制只抓取部分url
	open方法中的wbr：w：写、b：二级制、r：读
	需要使用cookie的网站在settings.py中配置如下后，第一个scrapy中会进行cookie传递
		COOKIES_ENABLED=True #网站需要用到cookie
		COOKIE_DEBUG=True #控制台打印cookie
	思路：在start_request中用selenium模拟登录获取cookie，以后都可以使用了
	cookies=browser.get_cookies()
	scrapy认识的cookie转换：通过selenium保存的cookie默认给scrapy是识别不了的，所以要做此转换
		cookies_dict=[]
		for cookie in cookies:
            cookies_dict[cookie['name']] = cookie['value']
	判断路径文件是否存在：
		if os.path.exists('全路径/xxx.json'):
	去除html标签：
		from w3lib.html import remove_tags  #remove_tags就是我们要的方法

37、phantomJS：
	它是一个无界面浏览器，在linux和centos中使用，性能没有selenium高，多线程下性能降低
	下载界面：https://phantomjs.org/download.html
	使用：
		browser = webdriver.PhantomJS(executable_path="E:/python/phantomJS.exe")
		browser.get("http://www.html")
		browser.quit()
	phantomJS也可以设置为不加载图片

打码平台：
	云打码：识别率很高。http://www.yundama.com/demo.html
	超级鹰：验证码类型较多。http://www.chaojiying.com/
	
开发小技巧：
	list的join："".join(sel.xpath("xx").extract()).strip() #当有html中有回车换行时，要取一个文本可以用它把list连接起来，然后strip一下
	方法注释：在方法名下面，连续3个双引号，然后回车
	
	
项目部署
	项目官网：https://github.com/scrapy/scrapyd
	文档说明：https://scrapyd.readthedocs.io/en/stable/
	在虚拟环境中安装scrapyd：pip install scrapyd  #安装成功后在虚拟环境目录下的Scripts目录下出现一个scrapyd.exe
	创建服务器文件夹：E:\python\scrapyd
	切换到scrapyd目录后运行scrapyd：直接输入 scrapyd #它是服务器，然后就可以看见里面有个dbs目录，最开始是空的
	访问检查服务是否开启：http://127.0.0.1:6800 #点Jobs查看是否正常
		报错：web.Server Traceback (most recent call last): builtins.AttributeError: 'int' object has no attribute 'splitlines' 
		解决方案：尝试将Twisted 版本重新安装成 18.9.0
			卸载Twisted命令：pip uninstall Twisted
			安装指定版本Twisted：pip install Twisted==18.9
			
	安装客户端：pip install scrapyd-client #用于把本地的scrpy爬虫部署到scrapyd服务中
		#不用在虚拟环境中安装，python的安装目录下的Scripts目录中就会有scrapyd-deploy文件
		#建议在虚拟环境中安装，这样在虚拟环境中的Scripts目录中就会有scrapyd-deploy文件
	修改项目的scrapy.cfg文件：
		[deploy] 修改为：[deploy:qichacha]
		打开url的注释
	mac和linux下：在虚拟环境中执行scrapyd-deploy就可以运行
	windows下：
		在虚拟环境的目录的Scripts目录下创建一个名字叫scrapyd-deploy.bat文件，内容如下：
		@echo off
		
		"E:\python\Envs\qichacha_spider\Scripts\python.exe" "E:\python\Envs\qichacha_spider\Scripts\scrapyd-deploy" %1 %2 %3 %4 %5 %6 %7 %8 %9
		在虚拟环境中运行：scrapyd-deploy #就可以看见可以用了
	运行：scrapyd-deploy -l #查看到scrapy.cfg文件中配置的信息，如果报错则修改文件
	项目目录下打包发布：scrapyd-deploy qichacha -p QichachaSpider
	然后查看：E:\python\scrapyd就可以看见有eggs目录，里面就是我们的爬虫项目
	API说明：https://scrapyd.readthedocs.io/en/stable/api.html  #下面的api都是从这里查的
	打开cmder 运行：curl http://localhost:6800/daemonstatus.json #看项目，curl的都是在cmder中使用的
	启动任务调度：curl http://localhost:6800/schedule.json -d project=QichachaSpider -d spider=nologincompany  #运行myproject项目下的somespider爬虫
		如果项目报错TypeError: __init__() got an unexpected keyword argument '_job'，就去修改该spider文件的init方法，添加一个**kwargs参数
	删除任务：curl http://localhost:6800/delproject.json -d project=QichachaSpider
	取消任务：curl http://localhost:6800/cancel.json -d project=QichachaSpider -d job=0b7cd0c688ef11e9a99c005056c00008  #job对应的就是jobid
	查看多少个projects：curl http://localhost:6800/listprojects.json
	查看多少个spider：curl http://localhost:6800/listspiders.json?project=myproject
	